{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### NOTE: Sometimes we refer to [the original work](http://nbviewer.jupyter.org/github/mathewzilla/redcard/blob/master/Crowdstorming_visualisation.ipynb) \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to install `scikit v0.18`: `conda update scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and minimal cleanup: for the first part, we need the labels (i.e. colour ratings), so we can't use the points where they don't exist. \n",
    "\n",
    "Since we will later `aggregate` the players, it is **important** to note that this doesn't produce inconsistencies because `dyads` is constructed by a join between a `players` table and a `referees` table, so it is natural that the missing values are missing for all instances of a player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dyads = pd.read_csv(\"CrowdstormingDataJuly1st.csv\", index_col=0)\n",
    "print(dyads.shape)\n",
    "\n",
    "dyads.dropna(subset=['rater1'], inplace=True)\n",
    "print(dyads.shape)\n",
    "\n",
    "# since both values are missing at the same time, this should be 0:\n",
    "print(dyads.rater2.isnull().sum())\n",
    "\n",
    "# the groupby object user later on\n",
    "group_players = dyads.groupby(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the numbers: \n",
    "\n",
    "<sub>yes, they were done by the other guys, but it's useful to have them at hand:</sub>\n",
    "\n",
    "Also, they were excluding some referees that have been _carried over_, and that only removes ~3% of the data. Since we're not doing statistics on referees, we won't drop them. Every little data helps :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of players: \" , dyads.index.unique().size)\n",
    "print(\"Number of referees: \", dyads.refNum.unique().size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original analysis mentioned that \"*the two raters disagree on 28742 or 19% of the time*\". Since there are only 1585 players, it means they ran it on the `dyads` set. _**WHY?**_ That doesn't make sense, so let's just check that the ratings for each player are consistent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the group of each player, we check that the number of values in `raterX` is **exactly** one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_player_consitency(player_df):\n",
    "    \"\"\" Needs to return a Series of {col_name: col_value}. \"\"\"\n",
    "    return pd.Series({col+\"_INconsistent\" : (player_df[col].unique().size != 1) \n",
    "                                             for col in ['rater1', 'rater2'] })\n",
    "consistency = group_players.apply(build_player_consitency)\n",
    "print(\"Rater1 has been inconsistent %d times\" % consistency.rater1_INconsistent.sum())\n",
    "print(\"Rater2 has been inconsistent %d times\" % consistency.rater2_INconsistent.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so they _ARE_ consistent. This means that their statistic doesn't account for players who have more matches than others, so the numbers are skewed. Let's check again, this time on _unique_ players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "player_ratings = group_players.agg({'rater1':'first', 'rater2':'first'})\n",
    "diffs = player_ratings.rater1 - player_ratings.rater2\n",
    "print(\"The raters disagree for {p:.3f}% of the players\".format(p=(diffs != 0).sum() / len(diffs) ))\n",
    "\n",
    "print(\"Diffs std dev: \", diffs.std())\n",
    "\n",
    "max_diff = diffs.abs().max()\n",
    "num_occur = (diffs.abs() == max_diff).sum()\n",
    "print(\"Max disagreement value {0}, occuring {1} times\".format(max_diff * 4, num_occur)) # *4 to pass from float to int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means:\n",
    "  1. that there is slightly more agreement between the raters for players who have more entries in `dyads` i.e. who played under more referees\n",
    "  2. that if we use both labels, using `accuracy` as a measure of performance is not a very good idea. Keep in mind that the differences are not ordered, so it could have an impact double as big on the accuracy, i.e. at most $1 - 2 * \\mathit{disagreementPercentage} = 1 - 2 * 0.24 \\approx 0.5 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Curiosity\n",
    "Who are the 'controversial' guys :) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "diffs[diffs.abs() == max_diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style='float:left' alt='Kyle-walker' src='http://www.thefootballsocial.co.uk/images/players/Tottenham%20Hotspur/Kyle%20Walker.jpg' /> <img alt='Mario_Goetze' src='http://i0.web.de/image/176/31756176,pd=2/mario-goetze.jpg' width=300/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate the data for each player we consider the following variables:\n",
    "- the height and weight of the player\n",
    "- The total amount of games played\n",
    "- The total amount of victories, ties and defeats\n",
    "- the total number of goals made\n",
    "- The total number of red cards, yellow reds and yellow cards received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "players = group_players.agg({'height':'first', 'weight':'first', 'games':'sum', \n",
    "                             'victories':'sum','defeats':'sum', 'ties': 'sum', 'goals':'sum', \n",
    "                             'redCards':'sum', 'yellowReds': 'sum', 'yellowCards':'sum'})\n",
    "print(players.shape)\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the weight or the height is NaN we replace it by the average height and weight of all the players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "av_height = players['height'].mean()\n",
    "av_weight = players['weight'].mean()\n",
    "players['height'] = players['height'].fillna(value=av_height)\n",
    "players['weight'] = players['weight'].fillna(value=av_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create extra features by normalizing the data:\n",
    "- The percentage of victories, ties and defeats\n",
    "- The number of red cards, yellow reds and yellow cards divided by the number of games played"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical_values = ['victories', 'ties', 'defeats', 'redCards', 'yellowReds', 'yellowCards']\n",
    "for name in categorical_values:\n",
    "    players['percentage_'+name] = players[name]/players['games']\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute extra feature based on correlation between mean IAT, cards given and mean Exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = group_players.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for racism in ['meanIAT', 'meanExp']:\n",
    "    for card in ['redCards', 'yellowCards', 'yellowReds']:\n",
    "        a = c.loc[c.index.get_level_values(1)==racism, card].reset_index(level=1).fillna(value=0)\n",
    "        players['cor_'+racism+card] = a[card]\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform categorical data into numerical values (example spain = 3) so that it can be used in random forest. We use:\n",
    "- club\n",
    "- country of the league\n",
    "- position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "categorical_values = ['club', 'leagueCountry', 'position']\n",
    "for name in categorical_values:\n",
    "    categorie = group_players.agg({name:'first'})\n",
    "    le.fit(categorie.as_matrix().flatten().tolist())\n",
    "    players[name] = le.transform(categorie.as_matrix().flatten().tolist())\n",
    "\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skin_color = group_players.agg({'rater1' : 'first'})\n",
    "skin_color.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1: predict player's skin color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the pandas data frame to lists in order to match the expected data format for scikit learn. We also map the player's skin color to an integer instead of a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = players.as_matrix()\n",
    "Y = skin_color.as_matrix().flatten()\n",
    "# map 0.25 to 1 etc\n",
    "Y = np.array(list(map((lambda x: x*4), Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the random forest using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4)\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=5, max_features=None)\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "    clf = clf.fit(X[train_index], Y[train_index])\n",
    "    # test model\n",
    "    Y_predict = clf.predict(X[test_index])\n",
    "    Y_predict2 = clf.predict(X[train_index])\n",
    "    print(\"accurancy predictions test data: \",(Y[test_index] - Y_predict).tolist().count(0) / len(Y_predict))\n",
    "    print(\"accurancy predictions training data: \",(Y[train_index] - Y_predict2).tolist().count(0) / len(Y_predict2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X, Y, scoring='accuracy', cv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "std = np.std([clf.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1,  players.columns[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure()\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
